{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea1471f",
   "metadata": {},
   "source": [
    "## Query Neptune Graph with SPARQL in Natural Language Using Function Calling\n",
    "\n",
    "This notebook will walk you through the steps to create a tool that can query Amazon Neptune database via SPARQL using function calling in Bedrock Converse API. The tool will be invoked when a user wants to get object litereal information from Amazon Neptune database for a specific object id. LLM(Large Language Model) will pass the required input parameter from the user's question the function *get_objectLiteral* and the function will execute the SPARQL query to fetch the results. At the end LLM(Large Language Model) will give the final response to the user including the query results.\n",
    "\n",
    "\n",
    "\n",
    "### Please Compete the Prerequisites before you start!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d67f8",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "**1- First we need to create our Neptune Database Cluster. Use *US-EAST-1* as the region.You can create cluster and all the required additional service configurations via**\n",
    "\n",
    "* Cloud formation template from this link [link](https://docs.aws.amazon.com/neptune/latest/userguide/get-started-cfn-create.html). Before you use AWS Cloudformation template make sure that you have the permissions described in [link](https://docs.aws.amazon.com/neptune/latest/userguide/get-started-prereqs.html)\n",
    "\n",
    "* After you deploy the CloudFormation Template, it will create\n",
    "\n",
    "    - The necessary IAM role for the Neptune Cluster\n",
    "    \n",
    "    - A new VPC,security group, subnets, route table, S3 Gateway Endpoint\n",
    "    \n",
    "    - Serverless Neptune Database\n",
    "\n",
    "\n",
    "**2- Creating the IAM role for the Sagemaker Notebook**\n",
    "\n",
    "* Go to Identity and Access Management(IAM)in AWS Console \n",
    "\n",
    "* Go to Neptune -> Clusters-> Your cluster being created via AWS CloudFormation \n",
    "\n",
    "* Click Role -> Create Role -> AWS Account -> and select the policies below to attach the Role\n",
    "\n",
    "  - *AmazonBedrockFullAccess*\n",
    "  \n",
    "  - *AWSCloudFormationFullAccess*\n",
    "  \n",
    "  - *NeptuneFullAccess*\n",
    "  \n",
    "  - *AmazonS3FullAccess*\n",
    "  \n",
    "* Click Trust Relationships and attach the json below into the relationships field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b64ce",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"NeptuneSagemakerNotebookAccess\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee465ff",
   "metadata": {},
   "source": [
    "* Create the role and save the role arn since you will use it in the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0cc9cc",
   "metadata": {},
   "source": [
    "\n",
    "**2- Creating the Sagemaker Notebook**\n",
    "\n",
    "* Go to Amazon Sagemaker in AWS Console\n",
    "\n",
    "* Click Notebooks-> Create Notebook instance\n",
    "\n",
    "  - Name the Notebook Instance\n",
    "  \n",
    "  - Choose *ml.t3.medium* as the instance type\n",
    "  \n",
    "  - Under **Permission and Encryption**  choose *Enter a custom role ARN* and paste the arn of the role that you have created in the previous step.\n",
    "  \n",
    "  - Under **Network** choose\n",
    "      - **VPC** -> 'neptune-test'\n",
    "      - **Subnet** -> Choose the subnet where your Neptune Cluster resides (You can check it from the Neptune console by clicking your Amazon Neptune Database writer instance under your Cluster -> Connectivity and Security -> Availability Zone\n",
    "      - **Security Group** -> Choose security group starting with 'Neptune'\n",
    "   - Click -> Create Notebook Instance\n",
    "   - Upload this notebook to jupyter lab that you can reach out when you click your notebook instance and pick the kernel as **conda_python3**\n",
    "\n",
    "\n",
    "### After you complete all the steps you should have\n",
    "* Amazon Neptune database \n",
    "* S3 VPC endpoint\n",
    "* IAM role for Neptune DB instance \n",
    "* IAM role for the notebook instance\n",
    "* VPC and network setup\n",
    "* Sagemaker Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907f0cf",
   "metadata": {},
   "source": [
    "## Running the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042cbf05",
   "metadata": {},
   "source": [
    "### It is important to use boto3 version equal or greater than 1.34.139!! Please make sure you execute the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4adf8-b8c6-4c08-b1df-dd11708aff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install the lates version of the boto3 library\n",
    "!pip3 install boto3==1.34.139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097001c-5e68-417c-bdcf-d31baadc0e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import boto3\n",
    "import json, sys\n",
    "from datetime import datetime\n",
    "\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5729a",
   "metadata": {},
   "source": [
    "### Configuring Amazon Bedrock for Model Access\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39568edd-8b92-4b58-bfa2-e98ae0ba41a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "#modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "#modelId = 'cohere.command-r-plus-v1:0'\n",
    "#modelId = 'cohere.command-r-v1:0'\n",
    "#modelId = 'mistral.mistral-large-2402-v1:0'\n",
    "print(f'Using modelId: {modelId}')\n",
    "\n",
    "region = 'us-east-1'\n",
    "print('Using region: ', region)\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6476ae5f",
   "metadata": {},
   "source": [
    "### Download the Human Disease Ontology Data\n",
    "\n",
    "In the cell below \n",
    "\n",
    "1- We will create an S3 bucket and download the Human disease ontology data into that bucket \n",
    "\n",
    "2- Replace bucket_name with your own Amazon S3 bucket name\n",
    "\n",
    "#### The dataset is publicly available under https://creativecommons.org/publicdomain/zero/1.0/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31912020-9c7d-4e1f-ab77-e63f92e20b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the bucket name\n",
    "bucket_name = 'BUCKET_NAME'\n",
    "\n",
    "# Create the S3 bucket\n",
    "response = s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "if response['ResponseMetadata']['HTTPStatusCode'] ==200:\n",
    "    print(f'Bucket {bucket_name} created successfully!')\n",
    "else:\n",
    "    print(f\"Failed to create the bucket Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d6767",
   "metadata": {},
   "source": [
    "### Download the dataset into the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the OWL file to download\n",
    "owl_file_url = \"http://purl.obolibrary.org/obo/doid.owl\"\n",
    "\n",
    "# Download the OWL file\n",
    "response = requests.get(owl_file_url)\n",
    "\n",
    "# Check if the download was successful\n",
    "if response.status_code == 200:\n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # S3 object key (filename in the bucket)\n",
    "    object_key = 'data/doid.owl'\n",
    "\n",
    "    # Upload the file to S3\n",
    "    s3_client.put_object(Body=response.content, Bucket=bucket_name, Key=object_key)\n",
    "    \n",
    "    s3_uri=f\"s3://{bucket_name}/{object_key}\"\n",
    "    \n",
    "    print(f\"File '{object_key}' uploaded to S3 bucket '{bucket_name}' successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f206a4e-6f96-4eb9-8cb9-417bafb6f452",
   "metadata": {},
   "source": [
    "### Now lets fetch our enpoints and IAM role ARN  from Neptune database. \n",
    "**Insert your *'DBClusterIdentifier'* name inside the parameters. You can find it from the neptune console.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605085a-64cd-4da1-97ba-cbfe181dd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neptune client\n",
    "neptune_client = boto3.client('neptune')\n",
    "\n",
    "# Define the parameters for the describe_db_clusters() operation\n",
    "params = {\n",
    "    'DBClusterIdentifier': 'YOUR_DATABASE_CLUSTER_NAME'\n",
    "}\n",
    "\n",
    "# Call the describe_db_clusters() operation\n",
    "response = neptune_client.describe_db_clusters(**params)\n",
    "\n",
    "# Extract the reader and writer endpoints from the response\n",
    "db_clusters = response['DBClusters']\n",
    "\n",
    "if db_clusters:\n",
    "    cluster = db_clusters[0]\n",
    "    reader_endpoint = cluster['ReaderEndpoint']\n",
    "    writer_endpoint = cluster['Endpoint']\n",
    "    arn_of_neptune_cluster_IAM_role=cluster['AssociatedRoles'][0]['RoleArn']\n",
    "    db_writer_instance_identifier=cluster['DBClusterMembers'][0]['DBInstanceIdentifier']\n",
    "\n",
    "    print(f'Neptune database reader endpoint: {reader_endpoint}')\n",
    "    print(f'Neptune database writer endpoint: {writer_endpoint}')\n",
    "    print(f'IAM Role ARN of the Neptune cluster: {arn_of_neptune_cluster_IAM_role}')\n",
    "    print(f'Neptune database writer instance identifier: {db_writer_instance_identifier}')\n",
    "else:\n",
    "    print('No Neptune database clusters found.')\n",
    "    \n",
    " \n",
    "# Define the parameters for the describe_db_instances() operation\n",
    "params2 = {\n",
    "    'DBInstanceIdentifier': db_writer_instance_identifier\n",
    "}\n",
    "\n",
    "\n",
    "response_instance_information = neptune_client.describe_db_instances(**params2)   \n",
    "\n",
    "db_writer_instance_endpoint=response_instance_information['DBInstances'][0]['Endpoint']['Address']\n",
    "print('endpoint:',db_writer_instance_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d9fc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Insert the writer endpoint of your database instance to check the connectivity, if it is not healthy please check the prerequisites again ! You can find the database endpoints from Amazon Neptune console\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80052c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking database connectivity\n",
    "!curl https://{writer_endpoint}:8182/status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ba83c",
   "metadata": {},
   "source": [
    "### Replace the parameters in this command with your own parameters for s3_uri and IAM role of the neptune cluster that allows S3 access\n",
    "\n",
    "curl -X POST \\\n",
    "\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    \n",
    "    https://your-neptune-endpoint:port/loader -d '\n",
    "    {\n",
    "    \n",
    "      \"source\" : \"s3://bucket-name/object-key-name\",\n",
    "      \n",
    "      \"format\" : \"format\",\n",
    "      \n",
    "      \"iamRoleArn\" : \"arn:aws:iam::account-id:role/role-name\",\n",
    "      \n",
    "      \"region\" : \"region\",\n",
    "      \n",
    "      \"failOnError\" : \"FALSE\",\n",
    "      \n",
    "      \"parallelism\" : \"MEDIUM\",\n",
    "      \n",
    "      \"updateSingleCardinalityProperties\" : \"FALSE\",\n",
    "      \n",
    "      \"queueRequest\" : \"TRUE\",\n",
    "      \n",
    "      \"dependencies\" : [\"load_A_id\", \"load_B_id\"]\n",
    "      \n",
    "    }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bde7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('s3_uri:',s3_uri)\n",
    "print('iamRoleArn:',arn_of_neptune_cluster_IAM_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e00bbc",
   "metadata": {},
   "source": [
    "### Copy the s3_uri and iamRoleARN above and paste in to the souce and iamRoleArn fields below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad105c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data from S3 to writer endpoint of your database instance\n",
    "!curl -X POST -H 'Content-Type: application/json' https://{db_writer_instance_endpoint}:8182/loader -d '{{\"source\" :\"s3_uri\",\"format\" : \"rdfxml\",\"iamRoleArn\" : \"IAM_ROLE\",\"region\" : \"us-east-1\",\"failOnError\" : \"FALSE\",\"parallelism\" : \"MEDIUM\",\"updateSingleCardinalityProperties\" : \"FALSE\",\"queueRequest\" : \"TRUE\"}}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get neptune query url\n",
    "neptune_port='8182'\n",
    "url=f\"https://{reader_endpoint}:{neptune_port}/sparql\"\n",
    "print('neptune_query_url: ',url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a197ff85",
   "metadata": {},
   "source": [
    "### Check the example query below before we define our tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test SPARQL query\n",
    "query = \"\"\"\n",
    "    PREFIX : <http://purl.obolibrary.org/obo/>\n",
    "    SELECT ?objectLiteral\n",
    "    WHERE {\n",
    "        :DOID_9884 ?p ?objectLiteral .\n",
    "        FILTER(isLiteral(?objectLiteral))\n",
    "        FILTER(STRSTARTS(STR(?p), 'http://purl.obolibrary.org/obo/'))\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "# Send the SPARQL query to the Neptune endpoint via POST request\n",
    "response = requests.post(url, data={\"query\":query})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba328409",
   "metadata": {},
   "source": [
    "### Defining our ToolsList with *get_objectLiteral* function which will execute the SPARQL query for the Amazon Neptune database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35a519-1b8b-4f78-ad12-18f148d33452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToolsList:\n",
    "    #Define our get_objectLiteral tool function...\n",
    "    def get_objectLiteral(self, id, predicate):\n",
    "        print(id, predicate)\n",
    "        query_url = url\n",
    "        query = f\"\"\"\n",
    "            PREFIX : <{predicate}> \n",
    "            SELECT ?objectLiteral \n",
    "            WHERE {{:{id} ?p ?objectLiteral . \n",
    "            FILTER(isLiteral(?objectLiteral)) \n",
    "            FILTER(STRSTARTS(STR(?p), '{predicate}'))\n",
    "            }}\"\"\"\n",
    "        print(query)\n",
    "        response = requests.post(\n",
    "            f\"{query_url}\",\n",
    "            data={\"query\": query},\n",
    "        )\n",
    "        result = f'Value in {id}, {predicate} is ' + response.text\n",
    "        print(f'Tool result: {result}')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcb527-eb7e-49b3-9116-8a7bae3bb504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the configuration for our tool...\n",
    "toolConfig = {'tools': [],\n",
    "'toolChoice': {\n",
    "    'auto': {},\n",
    "    #'any': {},\n",
    "    #'tool': {\n",
    "    #    'name': 'get_weather'\n",
    "    #}\n",
    "    }\n",
    "}\n",
    "\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'get_objectLiteral',\n",
    "            'description': 'Get objectLiteral value of a given ID.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'id': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'ID of the object'\n",
    "                        },\n",
    "                        'predicate': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'predicate'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['id', 'predicate']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86b457-bbb2-46df-9f70-17d14b38d13c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function for caling the Bedrock Converse API...\n",
    "def converse_with_tools(messages, system='', toolConfig=toolConfig):\n",
    "    response = bedrock.converse(\n",
    "        modelId=modelId,\n",
    "        system=system,\n",
    "        messages=messages,\n",
    "        toolConfig=toolConfig\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff7a01-3088-4a98-93bd-e184406deb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function for orchestrating the conversation flow...\n",
    "def converse(prompt, system=''):\n",
    "    #Add the initial prompt:\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Initial prompt:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "    #Invoke the model the first time:\n",
    "    output = converse_with_tools(messages, system)\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Output so far:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "    #Add the intermediate output to the prompt:\n",
    "    messages.append(output['output']['message'])\n",
    "\n",
    "    function_calling = next((c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c), None)\n",
    "\n",
    "    #Check if function calling is triggered:\n",
    "    if function_calling:\n",
    "        #Get the tool name and arguments:\n",
    "        tool_name = function_calling['name']\n",
    "        tool_args = function_calling['input'] or {}\n",
    "        \n",
    "        #Run the tool:\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Running ({tool_name}) tool...\")\n",
    "        tool_response = getattr(ToolsList(), tool_name)(**tool_args) or \"\"\n",
    "        if tool_response:\n",
    "            tool_status = 'success'\n",
    "        else:\n",
    "            tool_status = 'error'\n",
    "\n",
    "        #Add the tool result to the prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        'toolResult': {\n",
    "                            'toolUseId':function_calling['toolUseId'],\n",
    "                            'content': [\n",
    "                                {\n",
    "                                    \"text\": tool_response\n",
    "                                }\n",
    "                            ],\n",
    "                            'status': tool_status\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        #print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Messages so far:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "        #Invoke the model one more time:\n",
    "        output = converse_with_tools(messages, system)\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Final output:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\\n\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fcfa1c-3483-4abd-a20d-a12f6e54e0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the object literal for id as 'DOID_9884' and predicate as 'http://purl.obolibrary.org/obo/' \",\n",
    "    ]\n",
    "\n",
    "for prompt in prompts:\n",
    "    converse(\n",
    "        system = [{\"text\": \"You're provided with a few tools; \\\n",
    "            only use the tool if required. Don't make reference to the tools in your final answer.\"}],\n",
    "        prompt = prompt\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0631c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "In this notebook we learned how to create a tool that can query Amazon Neptune database via SPARQL using function calling in Bedrock Converse API.\n",
    "\n",
    "Our tool help you to get object literal value from Amazon Neptune database for a specific object id using natural language. User asks the object literal with a given id and LLM(Large Language Model) in this case 'Claude 3 Sonnet' passes the required input parameter from the user's input to the function get_objectLiteral and the function executes the SPARQL query to fetch the results. \n",
    "\n",
    "In our example, if you have a successfull execution you should be able to see the output text as:\n",
    "\n",
    "*\"The object literal value for DOID_9884 with the predicate http://purl.obolibrary.org/obo/ is \"A myopathy is characterized by progressive skeletal muscle weakness degeneration.\"\"*\n",
    "\n",
    "#### !!! This notebook has full permissions for S3 and Neptune Database for test purposes. In case you want to use this code example in prod make sure you restrict permission for your bucket and neptune database by specifying your bucket name and database id within IAM roles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d709a",
   "metadata": {},
   "source": [
    "## Clean Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8fe313",
   "metadata": {},
   "source": [
    "Lets Clean Up the resources being created. You can delete the Neptune Cluster and the VPC & Network resources by running the cell below.\n",
    "\n",
    "- For the 'stack_name' insert the name of the CloudFormation Stack that was being created at the beginning of this lab. You can find it through CloudFormation -> Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788cb32f-0070-497d-beb1-1004af28ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CloudFormation client\n",
    "cloudformation = boto3.client('cloudformation')\n",
    "\n",
    "stack_name = 'CLOUD_FORMATION_STACK_NAME'\n",
    "\n",
    "try:\n",
    "    cloudformation.delete_stack(StackName=stack_name)\n",
    "    print(f'Stack {stack_name} deletion initiated.')\n",
    "except Exception as e:\n",
    "    print(f'Error deleting stack {stack_name}: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d611e",
   "metadata": {},
   "source": [
    "**(Optional) You can delete your S3 bucket. Go to S3 -> Choose your bucket -> Delete**"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
